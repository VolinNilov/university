# Разработка Deep Q-Network для стабилизации маятника

## DQN (Deep Q-Network)

**DQN (Deep Q-Network)** — это алгоритм обучения с подкреплением (Reinforcement Learning), который сочетает в себе Q-обучение и глубокие нейронные сети. Он используется для решения задач, где агент взаимодействует с окружающей средой, чтобы максимизировать накопленную награду.

#### Основные принципы DQN:

1. **Q-функция**:
   - Q-функция (функция качества) оценивает, насколько хорошо выполнение действия \(a\) в состоянии \(s\) приведет к достижению целей.
   - Она определяется как:  
     
     \[
     Q(s, a) = \mathbb{E} \left[ \sum_{t=0}^{\infty} \gamma^t r_t | s_t = s, a_t = a \right]
     \]
    
     где:
        - \(s\) — текущее состояние,
        - \(a\) — выбранное действие,
        - \(r_t\) — награда на шаге \(t\),
        - \(\gamma\) — коэффициент дисконтирования.

2. **Нейронная сеть**:
   - В DQN Q-функция аппроксимируется глубокой нейронной сетью, которая принимает состояние \(s\) в качестве входных данных и выводит значения Q для всех возможных действий \(a\).

3. **Экспериментальный буфер (Experience Replay)**:
   - Агент сохраняет свои прошлые действия, состояния, награды и следующие состояния в буфере памяти.
   - Обучение происходит на случайно выбранных мини-батчах из этого буфера, что помогает стабилизировать процесс обучения.

4. **Целевая сеть (Target Network)**:
   - Для уменьшения колебаний при обучении используется вторая "целевая" нейронная сеть.
   - Она обновляется реже, чем основная сеть, и используется для расчета целевых значений Q.

5. **Обучение**:
   - Цель обучения — минимизировать разницу между текущими значениями Q и целевыми значениями:

     \[
     L = \mathbb{E} \left[ \left( r + \gamma \max_{a'} Q(s', a'; \theta^{-}) - Q(s, a; \theta) \right)^2 \right]
     \]
     
     где:
        - Целевая функция для текущей сети:
          \[
          Q(s, a; \theta)
          \]

        - Целевая функция для целевой сети:
          \[
          Q(s', a'; \theta^{-})
          \]

#### Применение DQN для стабилизации маятника:
- **Среда**: Используется, например, OpenAI Gym или аналогичная среда, где маятник должен быть стабилизирован в вертикальном положении.
- **Цель**: Агент обучается выбирать действия (например, приложение силы к маятнику), чтобы максимизировать награду (время удержания маятника в вертикальном положении).
- **Особенности**:
  - Непрерывное пространство состояний преобразуется в дискретное или обрабатывается через функцию политики.
  - Алгоритм постепенно улучшает стратегию управления маятником.

#### Преимущества DQN:
- Способность работать с высокоразмерными пространствами состояний.
- Эффективность в задачах с дискретным пространством действий.
- Стабильность обучения благодаря использованию экспериментального буфера и целевой сети.

#### Ограничения DQN:
- Не работает напрямую с непрерывными пространствами действий (для этого используются расширения, такие как DDPG или SAC).
- Требует тщательной настройки гиперпараметров.

## Ссылка на блокнот с кодом:
[Блокнот тут](https://colab.research.google.com/drive/1rM3-hVbK8wR5-_cPqneG79JtTt87L1YD?usp=sharing)