# Изучение TF Agents
## Обучение DQN-агента на задаче MountainCar

## Содержание
1. [Код обучения в Google Colab](#код-обучения-в-google-colab)
2. [Результат обучения на 10000 итераций и генерация видео основанной на 100 эпизодах](#результат-обучения-на-10000-итераций-и-генерация-видео-основанной-на-100-эпизодах)
3. [Результат обучения на 100000 итераций и генерация видео основанной на 100 эпизодах](#результат-обучения-на-100000-итераций-и-генерация-видео-основанной-на-100-эпизодах)
4. [Ответы на контрольные вопросы](#ответы-на-контрольные-вопросы)



### Код обучения в Google Colab
Код доступен [тут](https://colab.research.google.com/drive/1wx-zzvTblN3TFUeS1LBR7oYgto4fKCs6?usp=sharing)


### Результат обучения на 10000 итераций и генерация видео основанной на 100 эпизодах 
![10000 итераций и 100 эпизодов](https://github.com/VolinNilov/university/blob/main/MoAIiMR/4_lab_work/results/mountaincar_dqn_10000_100.gif)


### Результат обучения на 100000 итераций и генерация видео основанной на 100 эпизодах 
![100000 итераций и 100 эпизодов](https://github.com/VolinNilov/university/blob/main/MoAIiMR/4_lab_work/results/mountaincar_dqn_100000_100.gif)

### Результат обучения на 100000 итераций и генерация видео основанной на 1000 эпизодах 
![100000 итераций и 100 эпизодов](https://github.com/VolinNilov/university/blob/main/MoAIiMR/4_lab_work/results/mountaincar_dqn_100000_1000.gif)


### Ответы на контрольные вопросы
1) Для чего используется библиотека TF Agents?
    TF Agents — это библиотека, разработанная для упрощения создания, обучения и тестирования агентов с подкреплением (Reinforcement Learning, RL) в сочетании с TensorFlow. Она предоставляет готовые инструменты для работы со средами (например, OpenAI Gym), реализации алгоритмов RL и управления процессом обучения.

    Основные возможности TF Agents:
    Реализация сред: Поддержка стандартных сред (например, OpenAI Gym) и возможность создания собственных сред.
    Готовые агенты: Включает реализации популярных алгоритмов RL, таких как DQN, PPO, SAC и других.
    Инструменты для обучения: Буферы воспроизведения, драйверы для сбора опыта, метрики для оценки производительности.
    Интеграция с TensorFlow: Возможность использовать преимущества TensorFlow для эффективного обучения и распределенных вычислений.

2) Основные функции среды.
    Среда (environment) в RL моделирует задачу, которую должен решить агент. Она предоставляет интерфейс для взаимодействия агента с окружающим миром. Основные функции среды:

    reset():

    Инициализирует среду, возвращая начальное состояние.
    Вызывается в начале каждого эпизода.
    step(action):

    Принимает действие агента и возвращает следующее состояние, награду, флаг завершения эпизода и дополнительную информацию.
    Основной метод для взаимодействия агента со средой.
    observation_spec():

    Возвращает спецификацию пространства состояний (например, размерность вектора состояния или тип данных).
    action_spec():

    Возвращает спецификацию пространства действий (например, дискретное или непрерывное пространство).
    render() (опционально):

    Визуализирует текущее состояние среды (полезно для отладки или демонстрации).
    time_step_spec():

    Описывает структуру объекта TimeStep, который содержит текущее состояние, награду, флаг завершения и другую информацию.

3) Размерности вектора состояния?
    Размерность вектора состояния зависит от конкретной среды и задачи. В общем случае она определяется методом observation_spec() среды.

    Примеры размерностей:
    Дискретное пространство состояний:

    Например, в игре "Карты" состояние может быть представлено целым числом, указывающим текущую карту (размерность = 1).
    Пример: ArraySpec(shape=(), dtype=int32).
    Непрерывное пространство состояний:

    Например, в задаче стабилизации маятника состояние может включать угол, угловую скорость и положение (размерность = 3).
    Пример: ArraySpec(shape=(3,), dtype=float32).
    Изображения:

    Если состояние представлено изображением, размерность может быть (высота, ширина, каналы).
    Пример: ArraySpec(shape=(84, 84, 3), dtype=uint8).
    Размерность вектора состояния важна для настройки входного слоя нейросети агента.

4) Какие бывают политики?
    Политика (policy) определяет, как агент выбирает действия в зависимости от текущего состояния. В TF Agents политики могут быть как детерминированными, так и стохастическими.

    Основные типы политик:
    Детерминированная политика (Deterministic Policy):

    Для каждого состояния выбирается одно конкретное действие.
    Пример: Политика, которая всегда выбирает действие с максимальным значением Q(s, a).
    Стохастическая политика (Stochastic Policy):

    Действия выбираются на основе распределения вероятностей.
    Пример: Политика, которая выбирает действия с вероятностями, пропорциональными их ожидаемым наградам.
    ε-greedy политика:

    Сочетание исследования (exploration) и использования (exploitation).
    С вероятностью ε выбирается случайное действие, а с вероятностью (1 - ε) — действие с максимальным значением Q(s, a).
    Categorical Policy:

    Используется в алгоритмах типа C51, где политика моделирует распределение наград.
    Для каждого действия предсказывается распределение вероятностей по дискретным значениям наград.
    Gaussian Policy:

    Используется в задачах с непрерывным пространством действий.
    Действия выбираются из нормального распределения с параметрами (среднее и дисперсия), которые зависят от состояния.

**Заключение**: TF Agents предоставляет мощные инструменты для работы с RL-задачами, включая реализацию сред, агентов и политик. Понимание основных функций среды, размерностей состояний и типов политик позволяет эффективно создавать и настраивать RL-системы.
