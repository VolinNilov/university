# Methods of artificial intelligence in mechatronics and robotics - MoAIiMR
# Задания к лабораторным работам

## Содержание
1. [1 Лабораторная работа](#1-лабораторная-работа)
2. [2 Лабораторная работа](#2-лабораторная-работа)
3. [3 Лабораторная работа](#3-лабораторная-работа)
4. [4 Лабораторная работа](#4-лабораторная-работа)

## 1 Лабораторная работа 
1)Анотатор.Установка/разметка

    - Вариант_1: https://github.com/HumanSignal/labelImg
    - Вариант_2: https://github.com/pascalberski/YOLO-Annotation-Tool
    - Доп материал:https://www.visiongeek.io/2019/10/preparing-custom-dataset-for-training-yolo-object-detector.html

Задания
1) Разметить не менее 70 изображений (формат YOLO).
    
    1.1 Изображения не должны повторяться.
    
    1.2 Можно осуществить поиск в базах данных: Kaggle, amazon...
    
    1.3 Изображения должны содержать, как 1 класс, так и несколько.
    
    1.4 Провести аннотирование для неменее 3-х классов обьекта.
2) Варианты обьектов:
	
    1-Машины

    2-Собаки
	
    3-Кошки
	
    4-Пауки
	
    5-Мотоциклы
	
    6-Прицы
	
    7-Беспилотники
	
    8-Люди
	
    9-Акулы
	
    10-Облака
	
    11-Архитектура домов
	
    12-Стулья
	
    13-Окна
	
    14-Головные уборы
	
    15-Обувь	


## 2 Лабораторная работа
Реализовать неросеть распознавания обьектов по варианту.
Представить вычислительный эксперимент.
(колличество классов для распознавания должнобыть 2-3)

Использовать размеченные изображения в рамках первой практической работы.


## 3 Лабораторная работа
1. Разобраться в DQN для стабилизации маятника.(https://www.tensorflow.org/agents/tutorials/9_c51_tutorial?hl=ru)
2. Реализовать нейросеть из пункта 1.
3. Реализовать нейросеть для стабилизации используя любую другую сеть с подкреплением.
4. Сравнить сети из пунктов 2 и 3.
5. Контрольный вопросы :

	a) Назначение буффера воспроизведения , какие бывают?
	
    б) Как называются основные кофиценты нейросети с подкреплением и зачем они нужны?
	
    в) Обьясните принцып действия изученных вами нейросетей.
	
    г) Что такое метрика?
	
    д) Какие метрики используются для оценки качества  нейросетей с подкреплением?(назвать 2 или более)
	
    е) Какие нейросети с подкреплением подходят для работы в динамических средах?


https://www.tensorflow.org/agents/tutorials/2_environments_tutorial?hl=ru

### Ответы на контрольные вопросы

#### a) Назначение буфера воспроизведения, какие бывают?

**Буфер воспроизведения (Replay Buffer)** — это механизм хранения и выборки опыта (переходов состояний, действий, наград и следующих состояний), который используется для обучения нейросетей с подкреплением. Его основное назначение — разрыв корреляции между последовательными переходами в среде, что помогает стабилизировать обучение.

##### Основные типы буферов воспроизведения:
1. **Простой буфер воспроизведения (Uniform Replay Buffer):**
   - Хранит все переходы равномерно.
   - При выборке данных из буфера каждый переход имеет одинаковую вероятность быть выбранным.
   - Прост в реализации, но может быть неэффективным для сложных задач.

2. **Приоритетный буфер воспроизведения (Prioritized Replay Buffer):**
   - Переходы выбираются с учетом их "важности" (например, на основе ошибки TD).
   - Увеличивает вероятность выбора переходов, которые вносят больший вклад в обучение.
   - Повышает эффективность обучения, но требует дополнительных вычислительных ресурсов.

3. **N-шаговый буфер воспроизведения (N-step Replay Buffer):**
   - Хранит последовательности из N шагов, что позволяет лучше оценивать награды за несколько шагов.
   - Полезен для задач, где награды распределены во времени.

#### б) Как называются основные коэффициенты нейросети с подкреплением и зачем они нужны?

Основные коэффициенты (гиперпараметры) нейросетей с подкреплением:

1. **Скорость обучения (Learning Rate):**
   - Определяет, насколько сильно обновляются веса нейросети на каждом шаге обучения.
   - Высокая скорость может привести к нестабильности, а низкая — к медленному обучению.

2. **Дисконтирующий фактор (Discount Factor, γ):**
   - Определяет важность будущих наград по сравнению с текущими.
   - Значение γ ∈ [0, 1]. Чем ближе γ к 1, тем больше агент учитывает долгосрочные награды.

3. **Коэффициент исследования (Exploration Rate, ε):**
   - Используется в алгоритмах типа ε-greedy для управления балансом между исследованием (exploration) и использованием (exploitation).
   - Высокое значение ε увеличивает случайность действий, что полезно на начальных этапах обучения.

4. **Размер батча (Batch Size):**
   - Количество переходов, извлекаемых из буфера воспроизведения для одного шага обучения.
   - Влияет на стабильность и скорость обучения.

5. **Целевой коэффициент (Target Update Coefficient, τ):**
   - Используется для обновления целевой сети (target network) в алгоритмах DQN.
   - Малое значение τ обеспечивает плавное обновление целевой сети, что уменьшает колебания.

#### в) Объясните принцип действия изученных вами нейросетей.

##### DQN (Deep Q-Network):
- **Принцип действия:**
  - DQN использует нейросеть для аппроксимации функции Q(s, a), которая оценивает качество каждого действия в данном состоянии.
  - Агент взаимодействует со средой, собирая опыт (состояния, действия, награды, следующие состояния) и сохраняя его в буфере воспроизведения.
  - Для обучения используется мини-батч из буфера, где цель (target) вычисляется с помощью целевой сети:
    $$
    y = r + \gamma \max_{a'} Q(s', a'; \theta_{\text{target}})
    $$
  - Веса основной сети обновляются с использованием градиентного спуска для минимизации ошибки между предсказанными и целевыми значениями Q.

##### C51 (Categorical DQN):
- **Принцип действия:**
  - C51 расширяет DQN, моделируя распределение наград вместо их ожидаемого значения.
  - Вместо одного значения Q(s, a) для каждого действия, C51 предсказывает распределение вероятностей наград для фиксированного набора дискретных значений.
  - Это позволяет учитывать неопределенность наград и улучшает обучение в задачах с высокой изменчивостью наград.

##### PPO (Proximal Policy Optimization):
- **Принцип действия:**
  - PPO — это алгоритм политик, который напрямую оптимизирует стратегию (policy) агента.
  - Вместо использования функции Q, PPO максимизирует ожидаемую награду, обновляя параметры политики с ограничением на размер шага (clip range).
  - Это предотвращает слишком большие изменения в политике и делает обучение более стабильным.

#### г) Что такое метрика?

Метрика — это числовая характеристика, используемая для оценки качества работы модели или алгоритма. В контексте нейросетей с подкреплением метрики помогают понять, насколько хорошо агент решает задачу, и как он улучшается с течением времени.

#### д) Какие метрики используются для оценки качества нейросетей с подкреплением? (Назвать 2 или более)

1. **Суммарная награда (Cumulative Reward):**
   - Средняя сумма наград, полученных агентом за эпизод.
   - Чем выше значение, тем лучше агент справляется с задачей.

2. **Скорость обучения (Learning Curve):**
   - График зависимости суммарной награды от числа эпизодов или шагов.
   - Показывает, насколько быстро агент обучается.

3. **Количество успешных эпизодов (Success Rate):**
   - Доля эпизодов, в которых агент достиг цели (например, стабилизировал маятник).

4. **Значение функции потерь (Loss):**
   - Ошибка между предсказанными и целевыми значениями (например, в DQN).
   - Позволяет оценить стабильность обучения.

#### е) Какие нейросети с подкреплением подходят для работы в динамических средах?

Для работы в динамических средах подходят следующие нейросети с подкреплением:

1. **DQN (Deep Q-Network):**
   - Подходит для задач с дискретным пространством действий.
   - Может адаптироваться к изменениям в среде благодаря механизму исследования (exploration).

2. **PPO (Proximal Policy Optimization):**
   - Эффективен для задач с непрерывным пространством действий.
   - Благодаря ограничению на шаг обновления политики, PPO более устойчив к изменениям в среде.

3. **SAC (Soft Actor-Critic):**
   - Алгоритм, который максимизирует как награду, так и энтропию (разнообразие действий).
   - Хорошо работает в динамических средах, где требуется баланс между исследованием и использованием.

4. **A3C (Asynchronous Advantage Actor-Critic):**
   - Использует несколько параллельных агентов для сбора опыта, что делает его устойчивым к изменениям в среде.
   - Подходит для задач реального времени.


## 4 Лабораторная работа

### Изучение TF Agents
1) Разобраться в примере реализацции среды для обучения агента.
2) Реализовать свою среду на основе любой простой игры.
3) Проверить работоспособность среды с помошью теста.
4) Реализовать драйвер, агента и запустить со средой. 

https://www.tensorflow.org/agents/tutorials/2_environments_tutorial?hl=ru

### Ответы на контрольные вопросы

#### 1) Для чего используется библиотека TF Agents?

**TF Agents** — это библиотека, разработанная для упрощения создания, обучения и тестирования агентов с подкреплением (Reinforcement Learning, RL) в сочетании с TensorFlow. Она предоставляет готовые инструменты для работы со средами (например, OpenAI Gym), реализации алгоритмов RL и управления процессом обучения.

##### Основные возможности TF Agents:
- **Реализация сред:** Поддержка стандартных сред (например, OpenAI Gym) и возможность создания собственных сред.
- **Готовые агенты:** Включает реализации популярных алгоритмов RL, таких как DQN, PPO, SAC и других.
- **Инструменты для обучения:** Буферы воспроизведения, драйверы для сбора опыта, метрики для оценки производительности.
- **Интеграция с TensorFlow:** Возможность использовать преимущества TensorFlow для эффективного обучения и распределенных вычислений.



#### 2) Основные функции среды.

Среда (environment) в RL моделирует задачу, которую должен решить агент. Она предоставляет интерфейс для взаимодействия агента с окружающим миром. Основные функции среды:

1. **`reset()`**:
   - Инициализирует среду, возвращая начальное состояние.
   - Вызывается в начале каждого эпизода.

2. **`step(action)`**:
   - Принимает действие агента и возвращает следующее состояние, награду, флаг завершения эпизода и дополнительную информацию.
   - Основной метод для взаимодействия агента со средой.

3. **`observation_spec()`**:
   - Возвращает спецификацию пространства состояний (например, размерность вектора состояния или тип данных).

4. **`action_spec()`**:
   - Возвращает спецификацию пространства действий (например, дискретное или непрерывное пространство).

5. **`render()`** (опционально):
   - Визуализирует текущее состояние среды (полезно для отладки или демонстрации).

6. **`time_step_spec()`**:
   - Описывает структуру объекта `TimeStep`, который содержит текущее состояние, награду, флаг завершения и другую информацию.



#### 3) Размерности вектора состояния?

Размерность вектора состояния зависит от конкретной среды и задачи. В общем случае она определяется методом `observation_spec()` среды.

##### Примеры размерностей:
1. **Дискретное пространство состояний:**
   - Например, в игре "Карты" состояние может быть представлено целым числом, указывающим текущую карту (размерность = 1).
   - Пример: `ArraySpec(shape=(), dtype=int32)`.

2. **Непрерывное пространство состояний:**
   - Например, в задаче стабилизации маятника состояние может включать угол, угловую скорость и положение (размерность = 3).
   - Пример: `ArraySpec(shape=(3,), dtype=float32)`.

3. **Изображения:**
   - Если состояние представлено изображением, размерность может быть `(высота, ширина, каналы)`.
   - Пример: `ArraySpec(shape=(84, 84, 3), dtype=uint8)`.

Размерность вектора состояния важна для настройки входного слоя нейросети агента.



#### 4) Какие бывают политики?

Политика (policy) определяет, как агент выбирает действия в зависимости от текущего состояния. В TF Agents политики могут быть как детерминированными, так и стохастическими.

##### Основные типы политик:

1. **Детерминированная политика (Deterministic Policy):**
   - Для каждого состояния выбирается одно конкретное действие.
   - Пример: Политика, которая всегда выбирает действие с максимальным значением Q(s, a).

2. **Стохастическая политика (Stochastic Policy):**
   - Действия выбираются на основе распределения вероятностей.
   - Пример: Политика, которая выбирает действия с вероятностями, пропорциональными их ожидаемым наградам.

3. **ε-greedy политика:**
   - Сочетание исследования (exploration) и использования (exploitation).
   - С вероятностью ε выбирается случайное действие, а с вероятностью (1 - ε) — действие с максимальным значением Q(s, a).

4. **Categorical Policy:**
   - Используется в алгоритмах типа C51, где политика моделирует распределение наград.
   - Для каждого действия предсказывается распределение вероятностей по дискретным значениям наград.

5. **Gaussian Policy:**
   - Используется в задачах с непрерывным пространством действий.
   - Действия выбираются из нормального распределения с параметрами (среднее и дисперсия), которые зависят от состояния.



##### Заключение:
TF Agents предоставляет мощные инструменты для работы с RL-задачами, включая реализацию сред, агентов и политик. Понимание основных функций среды, размерностей состояний и типов политик позволяет эффективно создавать и настраивать RL-системы.
